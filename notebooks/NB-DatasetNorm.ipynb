{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543c619f",
   "metadata": {},
   "source": [
    "## On Measuring Excess Capacity in Neural Networks\n",
    "\n",
    "Florian Graf, Sebastian Zeng, Bastian Rieck, Marc Niethammer and Roland Kwitt\n",
    "\n",
    "### Dataset Norm\n",
    "\n",
    "This notebook computes the training data norms for all CIFAR10/100 and Tiny-ImageNet-200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478d09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "\n",
    "PARENT_DIR = os.path.abspath(os.path.join('..'))\n",
    "if PARENT_DIR not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.misc import get_ds_and_dl\n",
    "from utils.tiny import TinyImageNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "775c6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = {\n",
    "    'cifar10': {\n",
    "        'data_dir': '../data'\n",
    "    },\n",
    "    'cifar100': {\n",
    "        'data_dir': '../data'\n",
    "    },\n",
    "    'tiny-imagenet-200': {\n",
    "        'data_dir': '../data'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d5b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_norm(dl):\n",
    "    s = 0\n",
    "    norms = []\n",
    "    for img,lab in dl:\n",
    "        n = img.norm(p=2,dim=[1,2,3])**2\n",
    "        norms += n.tolist()\n",
    "        s += img.norm(p=2)**2\n",
    "    return np.sqrt(np.sum(norms)), np.sqrt(s).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "291a2d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cifar10             : ||X||=12411.64\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cifar100            : ||X||=12390.75\n",
      "tiny-imagenet-200   : ||X||=42840.31\n"
     ]
    }
   ],
   "source": [
    "for ds_name, info in dataset_info.items():\n",
    "    ds_trn, ds_tst, dl_trn, dl_tst, num_classes = get_ds_and_dl(\n",
    "        ds_name, \n",
    "        info['data_dir'], \n",
    "        batch_size=128, \n",
    "        num_workers=4, \n",
    "        limit_to=-1, \n",
    "        randomize=False)\n",
    "    \n",
    "    v0,v1 = compute_dataset_norm(dl_trn)\n",
    "    print('{:20s}: ||X||={:.2f}'.format(\n",
    "        ds_name, v0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
